---
layout: post
title: 香农熵、交叉熵和KL散度之间的联系
tags: [Entropy, Cross Entropy, KL divergence]
mathjax: true
excerpt: "信息量的核心特征是什么？为什么事件概率越小，蕴含的信息量越大？独立事件的信息量又为何能直接相加？本文深入解析香农熵、交叉熵与 KL 散度的底层逻辑，揭示三者在信息度量与模型优化中的内在关联，帮你理清机器学习中核心信息论概念的脉络。"
author: liuyu
---

## 信息量
对于一个事件来说，其信息量有三个特征：
1. 发生的概率越小，表示其蕴含的信息量越大
2. 发生的概率越大，表示其蕴含的信息量越小
3. 对于独立的事件来说，它们的信息量是可以相加的

因此，信息量的定义如下所示：

![1](/assets/img/2025-08-13-kullback-leibler-divergence-detail/1.png){: .mx-auto.d-block :}

1. 一个事件发生概率的倒数取log即为一个事件的信息量。  
2. 式中概率的倒数表示信息量与事件的概率成反比。  
3. 取log是由于独立事件的联合概率为各个事件发生概率的乘积，取log可以使得独立事件的信息量变为相加。

![2](/assets/img/2025-08-13-kullback-leibler-divergence-detail/2.png){: .mx-auto.d-block :}

## 香农熵
香农熵的概念不是针对一个事件来说，而是针对一个概率分布，其表示一个概率分布包含的平均信息量，也就是概率分布的熵。和物理学中的熵定义略有不同。

香农熵是描述一个概率分布不确定性的一个方法。香农熵的定义是一个概率分布的期望平均信息量，如下所示：

![3](/assets/img/2025-08-13-kullback-leibler-divergence-detail/3.png){: .mx-auto.d-block :}

下面看一个例子：

![4](/assets/img/2025-08-13-kullback-leibler-divergence-detail/4.png){: .mx-auto.d-block :}

通过以上计算我们可以看到，对于概率分布来说，如果它的概率密度函数更加均匀，那么它产生的随机变量的不确定性更高，熵更大；如果它的概率密度函数更加聚拢，那么它产生的随机变量的确定性更高，熵更小。
## 交叉熵
假设有一枚硬币，其正面朝上和反面朝上的概率均为0.5，真实概率分布为p，此时对硬币的朝向概率进行估计，估计正面朝上概率为0.2，反面朝上概率为0.8，估计概率分布为q。给定估计概率分布q，使用估计概率分布q对真实概率分布p的平均信息量的估计即为交叉熵。

![5](/assets/img/2025-08-13-kullback-leibler-divergence-detail/5.png){: .mx-auto.d-block :}

每个事件的信息量使用估计概率分布q计算，事件发生的概率实际是以真实概率分布p来发生的，因此交叉熵为事件真实概率分布p乘以事件估计概率分布q求得的信息量。信息量是以估计概率分布q进行计算得到的，因为我们只能看到估计到的概率分布。

![6](/assets/img/2025-08-13-kullback-leibler-divergence-detail/6.png){: .mx-auto.d-block :}

交叉熵的值大于等于真实熵的值。概率分布相差越大，交叉熵越大，反之，概率分布相差越小，交叉熵越小。
## KL散度
KL散度是衡量两个概率分布的差异的函数，定义为交叉熵减去真实熵。假设p为真实概率分布，q为估计概率分布，则交叉熵由p和q计算得出，真实熵由p计算得出。

![7](/assets/img/2025-08-13-kullback-leibler-divergence-detail/7.png){: .mx-auto.d-block :}

KL散度大于等于0，当且仅当两个概率分布相等时，等号成立。
p和q的KL散度与q和p的KL散度不一样，因此KL散度不是衡量两个概率分布之间的距离的。

![8](/assets/img/2025-08-13-kullback-leibler-divergence-detail/8.png){: .mx-auto.d-block :}

假设估计概率分布q是 $$\theta$$ 的函数，最小化KL散度，对KL散度求梯度，由于p与 $$\theta$$ 无关，因此求导之后，KL散度的导数只剩下交叉熵的导数这一项，与 $$\theta$$ 无关的真实熵一项为0。
## 总结
1. 事件的信息量与事件的真实发生概率成反比。
2. 熵描述了一个概率分布的平均信息量。
3. 交叉熵从估计概率分布的角度描述了对真实概率分布的平均信息量的估计值。
4. KL散度定量描述了两个概率分布之间的区别，并且它是推导交叉熵损失函数的基础。
